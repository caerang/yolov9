{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import torch, yaml, cv2, os, shutil\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "from models.yolo import Model\n",
    "from utils.augmentations import letterbox\n",
    "from utils.general import xywh2xyxy, non_max_suppression\n",
    "from models.experimental import attempt_load\n",
    "from pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM, EigenCAM, HiResCAM, LayerCAM, RandomCAM, EigenGradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n",
    "\n",
    "# ModuleNotFoundError: No module named 'pytorch_grad_cam'\n",
    "# pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsAndGradients:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers, reshape_transform):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.handles = []\n",
    "        for target_layer in target_layers:\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_activation))\n",
    "            # Because of https://github.com/pytorch/pytorch/issues/61519,\n",
    "            # we don't use backward hook to record gradients.\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_gradient))\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        activation = output\n",
    "\n",
    "        if self.reshape_transform is not None:\n",
    "            activation = self.reshape_transform(activation)\n",
    "        self.activations.append(activation.cpu().detach())\n",
    "\n",
    "    def save_gradient(self, module, input, output):\n",
    "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
    "            # You can only register hooks on tensor requires grad.\n",
    "            return\n",
    "\n",
    "        # Gradients are computed in reverse order\n",
    "        def _store_grad(grad):\n",
    "            if self.reshape_transform is not None:\n",
    "                grad = self.reshape_transform(grad)\n",
    "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
    "\n",
    "        output.register_hook(_store_grad)\n",
    "\n",
    "    def post_process(self, result):\n",
    "        logits_ = result[:, 4:]\n",
    "        boxes_ = result[:, :4]\n",
    "        sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n",
    "        return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy()\n",
    "\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        model_output = self.model(x)\n",
    "        post_result, pre_post_boxes, post_boxes = self.post_process(model_output[0])\n",
    "        return [[post_result, pre_post_boxes]]\n",
    "\n",
    "    def release(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolov9_target(torch.nn.Module):\n",
    "    def __init__(self, ouput_type, conf, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ouput_type = ouput_type\n",
    "        self.conf = conf\n",
    "        self.ratio = ratio\n",
    "    \n",
    "    def forward(self, data):\n",
    "        post_result, pre_post_boxes = data\n",
    "        result = []\n",
    "        for i in trange(int(post_result.size(0) * self.ratio)):\n",
    "            if float(post_result[i].max()) < self.conf:\n",
    "                break\n",
    "            if self.ouput_type == 'class' or self.ouput_type == 'all':\n",
    "                result.append(post_result[i].max())\n",
    "            elif self.ouput_type == 'box' or self.ouput_type == 'all':\n",
    "                for j in range(4):\n",
    "                    result.append(pre_post_boxes[i, j])\n",
    "        return sum(result)\n",
    "\n",
    "class Yolov9XAI:\n",
    "    def __init__(self, weight_path, device, method, layer, backward_type, conf_threshold, ratio):\n",
    "        self.device = torch.device(device)\n",
    "        weight = torch.load(weight_path)\n",
    "        model_names = weight['model'].names\n",
    "        self.model = attempt_load(weight_path, device)\n",
    "\n",
    "\n",
    "class yolov9_heatmap:\n",
    "    def __init__(self, weight, device, method, layer, backward_type, conf_threshold, ratio, show_box, renormalize):\n",
    "        device = torch.device(device)\n",
    "        ckpt = torch.load(weight)\n",
    "        model_names = ckpt['model'].names\n",
    "        model = attempt_load(weight, device)\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.eval()\n",
    "        \n",
    "        target = yolov9_target(backward_type, conf_threshold, ratio)\n",
    "        target_layers = [model.model[l] for l in layer]\n",
    "        method = eval(method)(model, target_layers, use_cuda=device.type=='cuda')\n",
    "        method.activations_and_grads = ActivationsAndGradients(model, target_layers, None)\n",
    "\n",
    "        colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int)\n",
    "        self.__dict__.update(locals())\n",
    "\n",
    "    def post_process(self, result):\n",
    "        result = non_max_suppression(result, conf_thres=self.conf_threshold, iou_thres=0.65)[0]\n",
    "        return result\n",
    "    \n",
    "    def draw_detections(self, box, color, name, img):\n",
    "        xmin, ymin, xmax, ymax = list(map(int, list(box)))\n",
    "        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2)\n",
    "        cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2, lineType=cv2.LINE_AA)\n",
    "        return img\n",
    "\n",
    "    def renormalize_cam_in_bounding_boxes(self, boxes, image_float_np, grayscale_cam):\n",
    "        \"\"\"Normalize the CAM to be in the range [0, 1] \n",
    "        inside every bounding boxes, and zero outside of the bounding boxes. \"\"\"\n",
    "        renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n",
    "        for x1, y1, x2, y2 in boxes:\n",
    "            x1, y1 = max(x1, 0), max(y1, 0)\n",
    "            x2, y2 = min(grayscale_cam.shape[1] - 1, x2), min(grayscale_cam.shape[0] - 1, y2)\n",
    "            renormalized_cam[y1:y2, x1:x2] = scale_cam_image(grayscale_cam[y1:y2, x1:x2].copy())    \n",
    "        renormalized_cam = scale_cam_image(renormalized_cam)\n",
    "        eigencam_image_renormalized = show_cam_on_image(image_float_np, renormalized_cam, use_rgb=True)\n",
    "        return eigencam_image_renormalized\n",
    "    \n",
    "    def process(self, img_path, save_path):\n",
    "        # img process\n",
    "        img = cv2.imread(img_path)\n",
    "        img = letterbox(img)[0]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = np.float32(img) / 255.0\n",
    "        tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        try:\n",
    "            grayscale_cam = self.method(tensor, [self.target])\n",
    "        except AttributeError as e:\n",
    "            return\n",
    "        \n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.model(tensor)\n",
    "            pred = self.post_process(pred[0])\n",
    "        if self.renormalize:\n",
    "            cam_image = self.renormalize_cam_in_bounding_boxes(pred[:, :4].cpu().detach().numpy().astype(np.int32), img, grayscale_cam)\n",
    "        if self.show_box:\n",
    "            for data in pred:\n",
    "                data = data.cpu().detach().numpy()\n",
    "                cam_image = self.draw_detections(data[:4], self.colors[int(data[5])], f'{self.model_names[int(data[5])]} {float(data[4]):.2f}', cam_image)\n",
    "        \n",
    "        cam_image = Image.fromarray(cam_image)\n",
    "        cam_image.save(save_path)\n",
    "    \n",
    "    def __call__(self, img_path, save_path):\n",
    "        # remove dir if exist\n",
    "        if os.path.exists(save_path):\n",
    "            shutil.rmtree(save_path)\n",
    "        # make dir if not exist\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        if os.path.isdir(img_path):\n",
    "            for img_path_ in os.listdir(img_path):\n",
    "                self.process(f'{img_path}/{img_path_}', f'{save_path}/{img_path_}')\n",
    "        else:\n",
    "            self.process(img_path, f'{save_path}/result.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    params = {\n",
    "        'weight': '../experiments/gelan-e-1280-scratch-done/weights/gelan-e-1280-best-20240520.pt',\n",
    "        'device': 'cuda:0',\n",
    "        'method': 'XGradCAM', # GradCAMPlusPlus, GradCAM, XGradCAM, EigenCAM, HiResCAM, LayerCAM, RandomCAM, EigenGradCAM\n",
    "        'layer': [11, 14, 17],\n",
    "        'backward_type': 'all', # class, box, all\n",
    "        'conf_threshold': 0.2, # 0.6\n",
    "        'ratio': 0.02, # 0.02-0.1\n",
    "        'show_box': True,\n",
    "        'renormalize': False\n",
    "    }\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "gelan-e-sar summary: 690 layers, 57285011 parameters, 0 gradients, 188.6 GFLOPs\n",
      "Exception ignored in: <function BaseCAM.__del__ at 0x7f7e01a8e2a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/caerang/miniconda3/envs/torch/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py\", line 196, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'XGradCAM' object has no attribute 'activations_and_grads'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGradCAM.__init__() got an unexpected keyword argument 'use_cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43myolov9_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 33\u001b[0m, in \u001b[0;36myolov9_heatmap.__init__\u001b[0;34m(self, weight, device, method, layer, backward_type, conf_threshold, ratio, show_box, renormalize)\u001b[0m\n\u001b[1;32m     31\u001b[0m target \u001b[38;5;241m=\u001b[39m yolov9_target(backward_type, conf_threshold, ratio)\n\u001b[1;32m     32\u001b[0m target_layers \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mmodel[l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m layer]\n\u001b[0;32m---> 33\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m method\u001b[38;5;241m.\u001b[39mactivations_and_grads \u001b[38;5;241m=\u001b[39m ActivationsAndGradients(model, target_layers, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m colors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model_names), \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint)\n",
      "\u001b[0;31mTypeError\u001b[0m: XGradCAM.__init__() got an unexpected keyword argument 'use_cuda'"
     ]
    }
   ],
   "source": [
    "model = yolov9_heatmap(**get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**입력 이미지에 대해서 forward pass를 한 번 수행하고 검출 결과를 표시**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_grad_cam import EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
    "\n",
    "# TODO: yolov9에서 사용하는 prediction results format에 맞게 코드 수정\n",
    "def parse_detections(results):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**weights 파일 읽어서 모델 객체 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import check_img_size, Profile, increment_path\n",
    "from utils.dataloaders import LoadImages, IMG_FORMATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weights = '/work/src/yolov9/weights/gelan-e-1280-lion5-best-20240605.pt'\n",
    "data = '/work/src/yolov9/data/yolo-anatomy.yaml'\n",
    "imgsz = 1280\n",
    "half = True\n",
    "dnn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "gelan-e-sar summary: 690 layers, 57285011 parameters, 0 gradients, 188.6 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgsz: 1280, stride: 32, names: {0: 'item'}, model.pt: True\n"
     ]
    }
   ],
   "source": [
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(imgsz, s=stride)\n",
    "print(f'imgsz: {imgsz}, stride: {model.stride}, names: {model.names}, model.pt: {model.pt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단일 이미지 데이터 세트 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "vid_stride = 1\n",
    "# source = '/work/dataset/VR-DRONE-v1.0.0.test/20221207/1Class/30m/30c/Crop_0030_030m_30c_3_4_00344.jpg'\n",
    "source = '/work/dataset/vr-drone-test/images'\n",
    "dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.warmup(imgsz=(bs, 3, imgsz, imgsz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = '/work/src/yolov9/runs'\n",
    "name = 'debug'\n",
    "visualize = False\n",
    "exist_ok = True\n",
    "save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)\n",
    "augment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "dt = Profile(), Profile(), Profile()\n",
    "# im:<class 'numpy.ndarray'> (3, 736, 1280)\n",
    "for path, im, im0s, vid_cap, s in dataset:\n",
    "    # print(type(im), im.shape, len(im.shape))\n",
    "    with dt[0]:\n",
    "        im = torch.from_numpy(im).to(model.device)\n",
    "        im = im.half() if model.fp16 else im.float()\n",
    "        im /= 255\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]\n",
    "    with dt[1]:\n",
    "        visualize = increment_path(save_dir/Path(path).stem, mkdir=True) if visualize else False\n",
    "        print(visualize)\n",
    "        # # pred = [Tensor, [Tensor, Tensor, Tensor]]\n",
    "        # # pred.shapes = [torch.Size([1, 5, 19320]), [torch.Size([1, 65, 92, 160]) torch.Size([1, 65, 46, 80]) torch.Size([1, 65, 23, 40])]]\n",
    "        pred = model(im, augment=augment, visualize=visualize)\n",
    "        # print(len(pred), pred[0].shape, len(pred[1]), pred[1][0].shape, pred[1][1].shape, pred[1][2].shape)\n",
    "\n",
    "    # with dt[2]:\n",
    "    #     pred = non_max_suppression(pred, conf_thres, out_thres, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
